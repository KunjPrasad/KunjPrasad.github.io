<!--
    HTML book: Productionizing Backend Development, by Kunj Prasad on Github.
    Copyright (C) 2020  Kunj Prasad

    This program is free software: you can redistribute it and/or modify
    it under the terms of the GNU Affero General Public License as published
    by the Free Software Foundation, either version 3 of the License, or
    (at your option) any later version.

    This program is distributed in the hope that it will be useful,
    but WITHOUT ANY WARRANTY; without even the implied warranty of
    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
    GNU Affero General Public License for more details.

    You should have received a copy of the GNU Affero General Public License
    along with this program.  If not, see <https://www.gnu.org/licenses/>.
-->
<html lang="en">
	<head>
		<meta charset="utf-8">
		<title>Productionizing Backend Development</title>
		<meta name="description" content="Productionizing Backend Development HTML E-book">
		<meta name="author" content="Kunj Prasad">
		<meta name="viewport" content="width=device-width, initial-scale=1">
		<link rel="stylesheet" href="/utilities/styles-common.css">
		<link rel="stylesheet" href="styles.css">
		<style type="text/css"></style>
	</head>
	
	<body>
		<header>
			<h1>Productionizing Backend Development</h1>
		</header>
		<nav>
			<a id="1606665479" href="introduction.html">Introduction</a>
			<a id="1607048361" href="one-line-summary.html">One line summary</a>
			<a id="1606703843" href="personal-notes.html">Personal notes</a>
			<a id="1606665918" href="/utilities/disclaimer-generic.html">Disclaimer</a>
			<section>
				<h2>Glossary</h2>
				<a id="1608303678" href="glossary/req-resp.html">Request and response related terms</a>
				<a id="1608343012" href="glossary/req-handler.html">Request handler related terms</a>
				<a id="1608343369" href="glossary/security.html">Security related terms</a>
				<a id="1608344347" href="glossary/testing.html">Testing related terms</a>
				<a id="1608344358" href="glossary/housekeeping.html">Application housekeeping related terms</a>
				<a id="1608344368" href="glossary/infrastructure.html">Infrastructure related terms</a>
				<a id="1608344385" href="glossary/architecture.html">Architecture related terms</a>
			</section>
			
			
			
			<!-- TODO
			
			Add to glossary: ORM, Fao, function as first class, lazy evaluation. Loose coupled architecture
			
			Testing
			-- https://softwareengineering.stackexchange.com/questions/274937/is-it-bad-practice-to-make-methods-public-solely-for-the-sake-of-unit-testing
			-- create apt boundary where things are made public, and test those. This should go in unit testing. This is a trade-off between leaking implementation to break abstraction vs having loosely coupled architecture.
			-- add tests in proper folder. FOr controller related test, do so by making dummy server, and add in controller package. However, for multi functional test, add in separate package.
			-- Add validation as part of controller to not repeat it. Since controller tests are themselves made by sending dummy request, so do validation as part of it and not separate
			-- negative tests
			-- testing as means of documentation. test smallest section. Give proper name. Arrange, act, assert
			
			--always raise error instead of returning a non-200 response. Added benefit, can add auto logging in error handling!
			
			-- The key words "MUST", "MUST NOT", "REQUIRED", "SHALL", "SHALL NOT", "SHOULD", "SHOULD NOT", "RECOMMENDED", "NOT RECOMMENDED", "MAY", and "OPTIONAL" in this document are to be interpreted as described in BCP 14 RFC2119 RFC8174 when, and only when, they appear in all capitals, as shown here.
			
			-- profile based design to also help with data privacy -- gdpr
			
			https://stackoverflow.com/questions/38677889/difference-between-service-layer-and-controller-in-practice/38678061#38678061 -- Looking in Java view of controller, service, repository... controller holds interaction with request; service has business logic and no request based logic; repsitory has database logic and only interacts with service. Best to do everything in one transaction -- even the conversion from slugs into corresponding entity.
			--|---- For this reason, process serializers in Django should not do anything with child slug.. if you are using SlugRelated field, then keep it as read-only, or create-new.. don't modify. One flip side, if you are worried about the entity changing in span of request.. then best you are pessimistic locking and not rely on just using single transaction.
			
			-- Repository layer -- use it to add abstraction on data retrieval.. like, either using cache or DB, or saving file via different scheme (uri or http), or saving on local drives or saving in cloud
			--|---- If adding cache, ensure that you are updating cache and backend both.. do proper integration and functional testing
			
			SERVICE:
			-- First, identify/analyze the request response behavior. Don't start with db design. Link to "maintain DAG structure" in Advance use case.
			-- analyze requests with data ownership in mind. Start with data-ownership first design (good example on back forth communication storage, like in chat -- how to make it PII based - helps with take sanitize. Identify which party accesses which data). Ownership should flow down links. So, manyToMany is like a sink for PII and a good boundary between different roles. You must be very clear on ownership and backed by legal if you are going up up from manyToMany back. The conclusion you come to here must also match for corresponding audit log table
			-- PII should delete data, not relations.
			-- If getting multiple data by 3rd party api, store them in separate table and then link to it. This allows easier control if data policies change in future
			-- Model serializer vs process serializer. Even process serializer can be model state change (can be via patch) or a big process combining multiple models with child->parent DTO rather than parent->child structure. Also cover general validation requirements
			-- does childDTO come first or parentDTO. Answer changes based on model or process serializer - and this is confusing - because link is clear on entitys ide. Parent comes earlier. 
			-- if you want the DTO to show different data based on roles, that is set here. Have an explicit utility/method to convert DTO to/from entity.
			-- translation from slug/id to entity is best done before any validations start. Do analyze the concurrency expectations/failures.
			--  If you are making a POST/PUT call that takes a list entry (like doing bulk create/update, etc), then a good idea is to ensure that the list takes only a max number of entries. It is a sanity check to prevent someone from sending an absurdly high number of entries and stopping the processing.
			--|---- Also, in this case, since there is a list of entries, so you'll need to collect object slug for each entry. While validating, do referential checks on slug (..or maybe this needs to be do at model.clean(), and not under serializer!! - to keep proper separation of concerns -- or maybe at both places - to get a fail fast behavior!!??)
			--|---- As a good practice in REST design.. don't get a field from user that can be inferred.
			-- The current design of validation means that there is no scope for fuzziness! Conside the following: Say you're asking someone for a historical info (say their last name), and that this differs from their current last name (due to life event) - then what should you do? One way could be to add a flag of "force_commit" -- the idea being that generally this flag would be false and this would cause validation error. But if force_commit is true, then don't raise error. To communicate to UI, maybe send these errors by listing fields under force_commit, and then adding list of errors that can be bypassed therein. This serves to form a "WARNING" kind of UI behavior.
			-- If request body is of list type, then, when sending error, send it against the corresponding list index. This allows matching error with corresponding list entry that triggered it
			--|---- Always read slug from url - for get/put/patch calls.. don't read it from request body. For POST, it shouldn't be in request body.. but should be in response body.
			--|---- IMPORTANT: If your DTO fields, path param, etc. has constrainst, like can be a-z,A-Z,0-9,_, then also realize these constraints in the field definition. DOn't just take any arbitrary string. Take only the ones expected. This can be useful in preventing internal xss, i.e., say as part of your logic, you are logging these field values, etc. - and then are running some daily background task, possibly to collect some metric. Since these are external user data - you shouldn't be trusting it.. and some user may try to pass in a script that inadvertantly gets run. This is internal XSS. The point is - don't trust any data comingf rom out till you've verified it. Similar also holds for fields containing html text from user.. Best would be to just get markup (and define an enum type column which highlights that the entry being read is of markup type, not html).. but if you do need to store html, then escape it as you form the request body itself. 
			--|---- If storing file, don't use file-name in internal filepath. Use some uuid for internal filepath. One, this prevents anyone with acces sto filepath from understanding what file contains. Two, if you allow versions, then same filename can have multiple file path. Three, if you allow moving files, then you want to just change file name parent to signal that it has been moved.. not also worry about actually moving file, worrying about conflict, etc. ALSO.. either case, store file as encrypted on disk to prevent unwanted access.
			--|---- Let's say you have a model that can have nullable fields (implying that a certain event did not occur). When this model is serialized, when the field has values, it gets returned. When field does not have values, a null is returned, but the response json can be defined in such a way that "null" values are auto-dropped. This is a known convention that a json having a field with null value is same as json not having that field. But this is response json. Consider the flip side of request-json. Say, you want to update an existing model where you want to nullify the fields. You make a PATCH call. Should this have json request body with field name and a null value for it? Would this interfere with understanding that null valued field is as good as field being absent? You can say that you want asymmetry between how fields with null values are interpreted between request and response json. Now, consider if the request json is obtained by making call to some other service and by using its response json.. and we said that null values can be removed in response json. So, best, in such cases.. define a new field, like "nullify_{field}" in your DTO, such that if it is set, then you know that you must nullify the corresponding model fields.
			-- Is PATCH call a process serializer or model serializer???! Why? Maybe the answer to it also related to comment on not using "null" valued keys when wanting to nullify some fields of existing model.
			--|---- Let's say your request body takes a list type field which is required. What happens if user does not give data. The error response will show an object like error corresponding to this field. But if list is given and entries are wrong.. then it shows a list type error. Is this wrong? Should it be consistent -- It is correct. A question is would this make UI development difficult to consider both cases when showing error. The answer - NO, because your UI should always send a list type object and so, should cut out possibility of getting lobject type response
			--|---- Validation for a status type field vs it being polymorphic discriminating column -- or maybe put it in advance use case, since this needs to be understood from perspective of model also
			--|---- Ideally you should have same serializer class for POST, PUT, PATCH call (DELETE may not use serializer because instance data is deleted.. and since data is deleted, there's no use of returning a json, because there isn't any data to populate it!) -- this hooks on eing able to return same json for all calls.. improving ability to bypass using "If-Modified-Since"
			--|---- Ideally, if you are deserializing request body, then you got a POST, PUT, PATCH, DELETE call. In either case, you should not be having query Params. You can have path params and that defines some contextual object which may be needed by serializer. if so, pass those values in serializer context. If that is not feasible, then see if you can identify the calling view from serializer - in which case, you should define methods in calling view that reads the request and provides necessary objects. The goal of exercise is to prevent serializer from directly having access to request. This separates scope of view vs serializer. The serializer always deals with request/response body and that's it. The view now is the one that directly deals with the request, and if needed, reads values from there and passes to serializer. A related question to ask is whether it is necessary to send that extra data in serializer. If it is for validation, should the have been done in model.
			
			-- Now identify urls; what if there is an implciit context in url. 
			--|---- Using header, path param vs query param - think of it like directory. An advantage of using path param is that the parent/child relation is explicit -- having entries in query param don't make the parent child relation explicit; Query param good to present a search like behavior in flat list of items. On other hand, there may be contextual reasons to not add parent param in url path because that is unique (for user of certain role) and can be removed. Or, what if you want child resource but want to filter on multiple parent-id (matrix param is alternative, but that can get involved - can be at non terminal locations, see https://www.baeldung.com/spring-mvc-matrix-variables)  Related: see https://stackoverflow.com/questions/9169081/rest-apis-custom-http-headers-vs-url-parameters
			--|---- Realize that url with patha nd quer param gets bookmarked, but header is not saved. So, anything not related to respource but more on resource negotiation or authentication should be in header
			--|---- See https://stackoverflow.com/questions/389169/best-practices-for-api-versioning -- for URL versioning
			--|---- For users with a higher level of authorization use a more nested url. For another subset of user with a hierarchical less set of permission, maybe some values become implicit in context and so can be skipped - give them smaller url. One may say to just make url suited to lower hierarchy user and then add filter for higher hierarchy user.. but then, what guarantees that in future an even lower hierarchy won't come. Or, when just making higher-user, maybe you didn't think that a lower order user will come, so the full path REST structure would have looked good. There's also the idea of YAGNI. Why have filter params just with consideration of future low hierarchy user even when though they don't exist now. **EVEN MORE**: I would say that one should think of filter params as only when it is possible to have an array of values that can get passed to queryParam. If you have a queryParam that can always only take 1 value.. then it may be good to instead have it in path. ALSO NOTE: In terms of architecture, queryParams means you are doing filtering - so there is no parent-child relation.. which could be if there is a strong "sibling" relation. That means one cannot nicely form parent-child classifications and so should pull all data in and then filter...and then to enable hierarchical structure - instead have your views/controller logic route from one to other and setting values based on user-role for sub-user, or from query-param for super-user.. as you go from one view to another
			--|---- At any point, if you're returning a list that is unbounded, either as sub-field when doing a GET one object call, or if making a list call - that is not a good design. A good design always filters in some way or other to prevent sending unbounded list. Speaking generally, returning unbounded list at any point just tells that the design isn't hashed out on what to do once the list is obtained. Sending an unbounded list is giving a view of entire DB and that is not a good thing to do. Maybe at start.. but not for long term. Consider adding search and pagination. Also whenever returning, consider adding a default sort
			--|---- Ref checks and failures. Consider: GET /root/grandResource/{grandId}/parentResource/{parentId}/childResource/{childId}
			--|----|---- Without the final {childId} and if it's a GET call, then return 404 because the url should not exist. What about instead returning 200 with empty list.. I think it's not very REST like, because if reference checks failed, say, between {grandId} and {parentId}, then how did the list type url for child even get formed? It like saying "GET /root/grandResource/{grandId}/parentResource/{parentId}" is 404, but "GET /root/grandResource/{grandId}/parentResource/{parentId}/childResource" is 200 with empty list?!
			--|----|---- Without the {childId} and if it's a POST call, then, return a 404 if reference check fail in url. Note how this matches with above: Now, it won't even be the case that "GET /root/grandResource/{grandId}/parentResource/{parentId}/childResource" gives 200 with empty list but "POST /root/grandResource/{grandId}/parentResource/{parentId}/childResource" gives 404!!
			--|----|----|---- If reference check fails in the request object, then corresponding field should raise error and there be a 400 status.
			--|----|---- With the {childId} and if it's a GET, PUT, PATCH, DELETE call, then return empty list on reference check fail and 404 status
			
			--|---- Let's say you are dealing with a child resource under a parent resource. There are 2 ways to proceed in terms of URL:
			Option-1: "/root/parent/{parentSlug}" and "/root/parent/{parentSlug}/child/{childSlug}", or,
			Option-2: "root/parent/{parentSlug}" and "/root/child/{childSlug}".
			When to use one vs another vs maybe-both:
			--|----|---- First, remember to be flexible.. the decision you come to is very dependent on your application needs. Both cases hold their own merit.
			--|----|---- One difference is that when creating new child, first option will call "POST /root/parent/{parentSlug}/child" and second will call "POST /root/child". First must read parentSlug from url and not from request body, and second must read {parentSlug} from request body since none is available in url. If using 2nd, don't pass a filter-param to pass parentSlug.
			--|----|---- first option is suited if you will only query for a child after having gotten the parent and so you want to keep a referential integrity. Second is lax on that.. an attacker only needs to know the childSlug to get the resource. Note that this concern only holds during GET call. For POST, PUT, PATCH, and even DELETE, you can require a request-body with parentSlug in it.
			--|----|---- If using 2nd option, you can query for child resource from multiple parent resource at once, like, "GET /root/child?parent=slug1,slug2,slug3" (but can also use matrix params)
			--|---- If getting a child resource must always come with some implicit understanding of parentSlug, then Option-2 is better. Like, get child-resources for most-recent/active parent resource. Then "GET /root/child" would work but not first option until the parent slug is obtained via separate query. THAT BEING SAID.. even for second option, use something like "GET /root/child?parent=active" -- This leaves "GET /root/child" as idempotent in that it gives all child resources. The "parent=active" filter now says to get only most active parent. This way, different filters can be made for different use case. Maybe.. define your HATEOAS response to show different url based on different role!!!
			
			--|---- Make sperate url for different model=state-related or business-related specific actions.. rather than grouping them under 1 generic path that then adds a bunch of if-else!! Former is clearer to understand, better logging, better understanding of user intent by just looking at url. Related: See https://restful-api-design.readthedocs.io/en/latest/methods.html -- REST is generally about resources and CRUD operations on it. But sometimes, we may want to do some function on a resource. This article suggests that such operations should always be POST because they can be thought of as POSTing request to a message queue asking for an operation to get done.  Since every such messages are different, so it becomes a POST request. HOWEVER, I feel it can also be made as a PUT or PATCH if the operation is idempotent in nature.. or only causes partial changes.
			--|---- For single item url, do not add query param. It is weird to have a single item slug and then also a filter. If you feel the need to do so, then ask if the item that needs to be sent is buisness logic dependent or not. If yes, you want to send it in request body. If not, then send it in header. **This is a good way to see when to add field to request body, or query param or header
			
			**In general about REST: check out 4xx level status code and see if there are others that are more apt, rather than just sticking to 400 or 404. Few things to note:
			--|---- restrict 400 to if you're taking user data in request (hence POST, PUT calls), AND, if there's an error directly related to fields in request-data. When you raise validationError, don't send any other field in error response - it is confusing to the user on why an error is being reported on a field which they did not provide. If there is an error due to inter-relation between multiple fields, then raise an error corresponding to all those fields. This tells user to rectify multiple fields. Use a "non_field_error" for scenarios like if at top level request body fields, you were expecting at least one to be there, but none were. Or, if the user should have provided values for 2 fields, but instead provided just one. In this case, by reason above, don't raise error message for missing field. So, include that error under non_field_error (..to signify request object error, related to expected input fields in request body, but it being missing)
			--|---- If you have any other error during validation.. that is likely a 409 - because you were expecting a pre-condition before request processing but could not. Side note: Optimistic locking failures is a straight 409 response use case
			--|---- For ad-hoc actions, don't always make it a POST. If idempotent, make it PUT. Can also use PATCH
			--|---- Use 404 if the slugs in path param fail reference relation. Use 200 with empty list if url path params are ok, but due to query params there is no result. And remember, before you even start doing all this - do FULLY verify the user and whether they are authorized to see the result. If based on user role, additional constraints apply that are not captured in path param or query params, then add that extra. 
			--|---- If you application allows hard-delete with a marker identifying a past delete as done; or, even if you have soft delete but which cannot be reversed, then 410 Gone may be a better status. DO NOTE: 410 forces search engines to more quickly drop a page.
			
			-- Understand and use the http-cache directive, particularly for subset of url serving static data (like those from standard table)
			
			-- Now arrange data in normalized pakets, making tables. For this reason, never have entity and dto be same class
			--|---- Keep entity classes as much POJO, except add a method to store validations that you would have preferred at SQL level but couldn't be done due to language limitations.. can include polymorphic constraints, refrence validations (Why is it necessary? -- link to Django docs for serializer verification, and having it also do form validation). Since we're saying that reference validations are in lieu of sql constraints.. these should be raising IntegrityError, which the validation methods should catch and raise as ValidationError. Think of Model as a deserialized form of database table row. Concept of Single-responsibility now suggest not adding any extra meaning to model. So, I prefer leaving it in as simple form as possible. 
			--|---- Don't add any extra methods in model that reaks DAG. If you need DAG breaking logic, make separate utility classes.
			--|---- create class for common field names. Implementation differ in Java vs Python, it Java, include these as composition - don't do inheritance. In Django, add as a mixin.
			--|---- Referential checks should be for each table for which the current table contains a column as a foreign key.. no need to go any further. Handle null values accordingly
			--|---- NOTE: failure of unique constraint cannot be checked anywhere in the code without running in race conditions and is done first by DB only
			--|---- Standard data table (or should it be covered under DB Productionizing)
			--|---- Modular design means that one can have separate modules / repos where a portion of model is defined and then it is imported under the main project. When doing so, create an implementation-extension of the entity class in different modules and add it to main code, and then use those class as basis to have a unified DB migration file. Don't separate out migrations because in a scrum environment, it may not be clear on how relation between tables can evolve.. new parent tables can get made (particular standard data table), etc. - so, always create a copy of all DB tables involved in main app, and use that to make migrations, and define model level save() and validate() method. This also prevents validation logic to get distributed at multiple places - and keep it unified. 
			--|---- Producer vs consumer mindset in defining models. This is the first place where the difference shows up because all stages before it (defining response, request, url) are explicitly consumer focused. This is the first time where you go into an internal implementation where you may pick another library or module that then exists at par with other DB models you made. One common example is for authentication purpose, or say, audit tracking. (Not quite.. authentication - login / logout can also be from a common module) -- say more discussed later.. see advanced case of producer vs consumer
			--|---- consider using a framework, or adding a tempate yourself to help support aspect oriented design along database calls. This can be useful for audit tracking, object level security.
			--|---- What are considerations when designing a table for polymorphic entity? It can be modeled via a single table design with one column containing discriminator for different types. NOTE that what differentiates this from a db table for entity that can go through various stages, is that for the former, the enum valued column does not change its value after created. For the polymorphic table.. 
			--|----|---- add a separate validation constrain for each case. 
			--|----|---- Even more important question is how to design the table. Should it contain just one column which contains just the id and no explict foreign key link and the table to which that id corresponds to can change depending on polymorphic type.. or should there be an explicit foreign key dependency added? It depends on the use case. If what you are trying to do is (i) an aspect behavior and not a business behavior, (ii) deletion of original data does not cascade down to deletion of data in this table, (iii) you have no validation requirements on this table based on columns.. then you can store generic id without creating link, else always create link. An example of this is if you want to audit-track changes in model.
			--|----|---- -- When adding a new enum (or even a standard table entry), be careful of adding it to code. Because DB runs before application, the application won't be able to read new value. this can happen both if using standard data table or enum. Best do migration and code changes in separate deployment. If doing so does not fail in lower env, then it's ready to go for prod
			--|---- If a model has fields that override value of a preceeding model, then define custom getter such that it reads the overriden value if available, else read the original value.
			--|---- Use blank still, not null -- check with business requriements though. This does not extend to all fields that are string valued, for example, don't use empty datetime field instead of null.. because empty datetime field is an ill-formatted entry

 			-- Now denormalize data if needed. 
			-- Add wrappers around basic methods that allow DB level interaction. Add referential checks. Multi-referential cehck for denormalized data. Add constrainst that you couldn't do due to lack of SQL structure
			--|---- At some pont, also look at DB isolation levels. make sure you understand what's happening, specially if you have a highly concurrent system. For same reason, best to do any file processing, etc. earlier before starting transaction and limit transaction to not contain big processing steps. And for this reason, best to not setup all views to automatically be wrapped in a transaction - because you may need to limit when/where a transaction happens, and to have more than 1 transaction per request
			
			-- Identify service methods - which should be declaratve
			--|---- In earlier notes.. it is said to keep any logic inside a View / ViewSet as much declarative as possible. Any imperative logic should be properly moved in serializer, model or last.. in util. There is one more place where imperative logic can be moved -- in Mixins that are used with View. Also, recall that certain logic can be put in context_processor, or in middleware
			--|---- if some method is needed in multiple places.. it should be in viewset mixin.. in a utility class
			--|---- https://stackoverflow.com/questions/9271497/do-transactions-add-overhead-to-the-db : As much possible, prefer to do all changes in transaction. Also, don't do any long time taking task in a transaction. For example, if you want to save a file given by user, or process the file (say, convert from docx to pdf) - do all those before / after transaction but not within it.
			--|----|---- Above is an example of interaction with multiple data store. In such cases, always have a background sync / clenup process. Also design your code to be able to do so. Use Transactional outbox pattern.. or if your code is in initial or non-complex stages, then just a logging with a periodic check might also suffice.
			--|---- Best practice is to have one application interact with one DB. Don't have many applications interact with one DB. instead require  from other applications that they make an API call to one of them to get the response. Similarly, while it is possible for one application to interact with multiple DB.. it is best to instead break those to smaller-applications and maintain one-DB--one-application relation. This allows for better data modeling and separation. How should the other applications deal with service outage is something that is best discussed and identified.
			
			-- Identify request validators that should run first, before service call - link to a secuirty section that also says to check multi level resource relation
			-- Have some logic that is always needed - it goes in filter
			
			-- Exception handler (should they be the only one to log)
			--|---- Maybe have a threadlocal to every request. This can provide handle to request. Can also define variables that can store logging context.. like, different variables encounted during processing that should be collected so it can be logged. Other option could be to set attribute directly on request If using threadlocal via a filter/middleware - remember to clear it on exit -- regardless of whether there is an error or not. On that note.. when executing custom action endpoint that triggers some change in a particular model status, you can store both the past and new status and log it
			--|---- Create a logging section which collects all the tips / tricks related to logs made throughout the book. Put that at very end.
			
			-- DateTime field, always in UTC. Save timezone as needed - in separate field in data model. Also analyze whether the timezone store in one table applies to multiple other related tables.
			--|---- If you want date and time, then get datetime instead and also have timezone support and not silently remove it.
			
			-- Advance case: Follow the DAG. Link to Same in "Prod-Prod" as meaning don't give circular logic (i am true because i said so, and i said so because i am true), or disjointed logic (i am true but true does not mean true and does not mean false)
			-- Advanced case: Producer vs consumer
			--|---- In designing/coding - and particularly regarding models, REALIZE that the difference between producer and consumer mindset is and important difference to identify. When you are coding as producer, you want your codebase to be as compliant to SOLID, it'll be open for expansion, but different functionalities will be in different files and you'll have more "distributed" codebase. the drawback, maybe you may one change, but forget to make related change elsewhere because by design concept -- everything is open - so a change at one place doesn't/shouldn't affect other places. When coding as consumer, you purposefully accept that code won't go to someone else and you have full control over its consumption and API structure. You can add efficiency with this mindset. But this affects code resiliency. Most common, you'll run into import loops and/or will want to move some code to utils.. which brings up question of why not do that since beginning. STILL.. even in consumer mindset, remember to not break DAG!! -- things are easily factored as long as both app, module, sub-module level dag is not broken.. particularly removing order of __init__.py
			--|---- This is probably biggest design contention when you are producing and using the codebase in your application. The tensions come up because a group is both producer and consumer - so both viewpoints are correct. However, traditional viewpoints of quickness, efficiency, related features at one place because you're the only one so why proliferate code -- come up and may blindside the efforts. Do realize it comes with a future cost in that there can be future issues.. so do keep that in mind - particularly if in scrum workflow. Again, nothing that can't be changed - but it'll need extra time/money. Personally, having a producer mindset is better because it keeps development open towards future changes, but it causes a distributed code.
			--|---- **Thinking about producer vs consumer mindset is a good way to reason about having abstract classes in each project and then have them implemented in your project's home app. This way, the definitions in each sub-app gives a basic skeleton.. that you modify as necessary when adding it in home-app
			--|---- one aspect is whether they should contain method to define nonDB-related ASPECT-LOGIC (like fields that should be logged), field based object-access, or whether that logic should be in some other class. Having things are one place bloats model, makes model definition dependant on context (which is something that may be good if you are end consumer -- as it keeps all related code at one place and low chance of missing things, but not a provider), also you may get forced to scenarios that don't respect DAG!
			--|---- Thinking along lines of differences between producer and consumer mindset is also a good way to think whether the model should be kept as much POJO and related methods to moved to separate utility class, or if related methods be added inside the model itself. Former is a producer mindset and latter is a consumer mindset. The disadvantage of latter is that now, classes that lie higher up in DAG precedence can be made to have functions that apply on classes that come lower in DAG precedence. Then, it moves to having those special methods in separate utility.. then it becomes why not just put everything there.. or maybe DAG relation was previously not very defined, but it gets so at later time, and now the chain is broken.
			--|---- Where should validation lie when dealing with discrete valued "status" type fields in model-vs-serializer -- see above comment!!
			--|---- From above, it seems that producer should only give basic pojo classes. Actually, differentiation between the two depends on the amount of functionality you want to add. If you give max freedom of extension, then you won't be able to add function, or will give something too fragmented. A good trade-off is to give an interface and some sort of basic implementation.
			
			-- Advanced case: DAG structure for ManyToMany field: DTO, Entity, url, create/update/delete: what happens first and what follows
			-- Advanced case: multi-representation of same data: Resume as text and pdf; DTP, model, url, create/update/delete
			-- Using "If-Modified-Since". Should it bubble, i.e. when a child is changed, then also its parent entity is also updated (This also relates to keeping auditlog entry)? - I'd say no. This means that when showing a DTO, one should NOT show childDTO because that then breaks the meaning of parentDTO being non-modified. A workaround could be to simply return url where one can get childDTO
			--|---- This is a good argument on why a REST GET url should only returns resource and no parent/child DTO. returning any extra information mixes the update time for two and so, is like a caching strategy. Best, and always, add cache depending on your use case and not preliminarily. This also sets up a guideline in that your GET call should end up asking for a resource and only that resource (not parent/child) should be returned
			
			-- Advanced use case: Validation break down for serializer + form + model serializer + process serializer + Fail-fast: do validation as fast and as soon possible + keeping the view logic declarative. 
			--|---- Keep model level validation and save() separate.. and also provide a new methods that joins the two. 
			--|---- Understand why the need for separation -- This istaken from own Django notes but can help here (-- Model.clean() contains validations that apply at model level. Likely, this is the place to add referential checks, or any checks that should hold at DB level but likely isn't getting applied due to limitation of SQL language. On the other hand, Form.clean() is to clean the data that is read in the form from user-request. For example, if your form take a time-in, time-out.. then Form.clean() would contain validation like the date-time is not in future.. but Model.clean() will contain validation like time-in < time-out . Note how Form.clean() is user-data and business-process related, whereas Model.clean() is model-definition related. This also explain why `ModelForm` in Django calls `Model.clean()` in its `self._post_clean()` method, which is run separately after the form's `self.clean_form()` method ..and these are different from form's `full_clean()` which by its name, must do all sort of cleaning.)
			
			-- Advanced case: HATEOAS Internationalization and more via Governor Service. Say that you don't know of an existing framework which does it.. but would be good
			
			-- Why hiding db id is good
* https://stackoverflow.com/questions/396164/exposing-database-ids-security-risk -- best answer, it is a business intelligence security risk!
* https://stackoverflow.com/questions/9904396/is-it-a-bad-practice-to-expose-db-internal-ids-in-urls
			-- Above says to only use Slugs in url. OK - consider when you have a foreign key relation. Now, Since slugs are unique.. do you make a "Foreign key" relation using "id" or using "slug"?
			--|---- I'd say this is where having some understanding of what a "slug" is - would be helpful. See https://stackoverflow.com/questions/427102/what-is-a-slug-in-django -- which shows how slug can be a nifty url line. Now, one could think that, say, if you're writing comments on a post of a given "slug", you'll talk about it as comment on post "about that slug". So, from users perspective, the relation should use slug. This also matches the "idea" that "id" field is just an internal representation and "slug" is what user sees - so slug is what keeps "relations" between data! However, from DB/service perspective, you now have a choice - either you say that (a) you do a foreign key on slug, and return slug. The advantage: when returning slug foreign key, you won't have to do another query.. that value is already in the table. HOWEVER.. if you're looking from the perspective where slugs can become long text so that they look nice in url, then "indexing" over long string can get problematic, and it can be much faster to make foreign key relations using "id", have foreign key index made using "integer" valued id - and then join on it. In such cases, doing a foreign key on "id" is much more beneficial. There is an intermediary case, say, when you have tables with many foreign keys - in this case, even though an integer index lookup is fast, there may be many such lookups needed and that'll slow things down. 
			--|---- A RELATED HUGE ADVANTAGE: Now you can use slug for foreign key. This way, when a query is made on child table, you can return parent's slug back without doing a join query, because child table already has it. Furthermore, if you delve back into actual rationale behind having a slug (https://stackoverflow.com/questions/427102/what-is-a-slug-in-django) - you'll notice that "slug" is supposed to provide context to a resource, so it is natural that every other related resource references the parent-resource using "slug" field (rather than "id" field) - to maintain the contextual coverage. Note that doing so does NOT prevent you from going back and using "id" instead for foreign key - since slugs are unique.. and not changed once it is set (See next point)
			--|----|---- DO NOT DO ABOVE IF YOU'VE USED NATURAL KEY FOR SLUG - for same reason as you never use natural key for making "id" - because natural keys almost always change in span of product, and then you're left scrambling to change all tables ..or even worse, they go from being unique to non-unique.
			-- Disadvantages: 
			--|---- (1) Anyone knowing the "slug" of 1st table can now immediately know about the slugs used elsewhere :: BUT then.. having unknowable slugs shouldn't be your only security aspect
			--|---- (2) What if the structure started as parent > child relation, but later more tables' foreign key were added. NOTE: (a) If, when adding other foreign key, you change the slug to be like `{foreignKey#1|foreignKey#2|...}`, that's bad - don't change the slug once it is set. That being done.. sure the original intent of the table was to have a parent > child relation, but now it grew beyond it.. that's fine.. the "slug" field still remains unique - which it should by definition - nothing wrong with it.

			-- While validating: use 409 status, or others; raise 400 only for entries in request data
			
			-- Choose your test fixture utility such that the test dtaa it creates also verify the referential relations
			-- create mock for 3rd party api and background tasks and integration test it
			-- Adding constraints in design :: Both for DB and for validators, Start by adding as strong of a constraint as possible.. relax later if need comes. Don't start with a weak unique constraint
---- Note that same also applies when writing validators. If a validator holds for a field, then keep it as a field level validator and don't put on class yet. Maybe, in future you'll need different class level validator or will need separate field validator.
			-- Unique DB check - can only be at DB level. If you try to add add logic in service - that won't work for race condition. ..because between validation and addition of data.. some other request may have added data that would fail validation. Wrap these with  suitable api-exception in your codebase. Don't try to check/catch for uniqueness anywhere before because they will not be thread safe. DB is the only place that can do so Do look into isolation level for your DB connection.
			
			-- use switches to open/close features
			--|---- Implement switches via aspects
			--|---- Avoid using double negative in switch name
			--|---- When failing due to switch, log it.
			--|---- Have switch on service if the goal is to have switch be used experimentally and then always be on. Keep switch on model if feature needs to stay for long. If going for latter option, maybe also consider if you should instead just have different permission, and break table into different one-to-one subtables.
			--|---- Consider defining a role that is supersedes a switch. This can be useful for testing. Be careful defining users under it - and have it be very controlled.
			
			-- give importance to Serializer and model field name. Use "id", "text", "code", "name", "datetime", "date" suffix as needed ..and specially use "indicator" suffix for booleans -- if you call a field as "is_done", then it becomes confusing to identify the corresponding method : `is_is_done()` or `get_is_done()`?
			-- identify and use aspect oriented programming. Consier using framework that allows aspect at all level. aspect around model, view, serializer. (Aspect around view is likley filter, so nothing extra might be needed there)
			-- CIS-20 : 20 point guidelines for various aspects of software security released by Center for Internet Security (CIS): https://www.cisecurity.org/controls/cis-controls-list/
			-- 12 factor app development: https://12factor.net/
			-- Cloud-readiness: That involves not relying on session. So, authentication via cookie goes out and JWT comes in. Recall that JWT gives client side cloud-readines.. and this applies even if website is interacting with single domain
			-- If you are having different project that can talk with each other.. then make sure that can also be done in local. It can be needed later on to be able to have this feature.
			
			
			HATEOAS:
			1) In Hateoas links, apart from internationalization 
---- also give security detail : who all will be accessing the data. Is there any restriction on mutating data, etc. Maybe just give link to security policy
---- a question is if you can guide users into using your webpage/REST-url in suitable flow. Maybe each URL within each level can be numbered so that users are guided. For some who have already done part of work.. they can skip those portions and get to next step. Maybe give guidance on how a user can identify which step they should start with.. say by doing successive GET calls.

			2) If a field/fields can take enumerated value only, then give all possible combination.. or url to get combination possibility, along with what they mean. This helps user understand enum values which are not in their language. Maybe give different enum-options based on the language selected by user?!! and/or add a translator at beginning so that the json data is translated to one understood by server.. even though it starts from one that is understood by server
			
			-- HATEOAS or swagger, or both?! See https://stackoverflow.com/questions/54839672/difference-between-swagger-hateoas  
			--|---- If using Swagger, also set custom headers, authentication.. and also add unit tests to verify that swagger spec getting generated has expected values
			--|---- Since HATEOAS is REST version of a UI, try to return only as much related content as allowed for a role
			--|---- Don't show HATEOAS text on error, you might end up causing leaks if user is not allowed to see the text. The error message should be descriptive enough. Not sure though.. maybe there's better process. Like, can point to homepage or a support page.
			
			-- For Swagger: when reading model field description -- best if the plugin being used can do by reading DB comment. This keep DB field comment as only source of truth when defining field definition text.
			
			-- At a later stage... consider internationalizing your error response and your HATEOAS!! Maybe swagger also!
			
			
			Security:
			-- Each request, even from async tasks.. should be as a request from a user. background tasks come as request from system user with a certain correct role. Gives uniformity in handling the task, logging, etc.
			-- Look at OWASP guidelines
			-- Look for notes in spring security, like changing session after login, clearing session on logout, adding x-content-type-options: nosniff. 
			-- See https://blog.avast.com/why-is-ebay-port-scanning-my-computer-avast and https://nullsweep.com/why-is-this-website-port-scanning-me/ -- don't port scan. If you want to do so for security, then offer as a service and start/stop it when user asks. Don't do so in a hidden cryptic way
			-- Don't have/rely on workflow where the website domain changes. This way, if you are changing domain, you can let the user know of it. A common reason do change domain could be to access OAuth token on different server. One reason to prefer this rather than collect user details by yourself and make an internal api call is that latter duplicates the login form code. Best, just let that single service evolve its own code/UI and you just get access to it. Also, since its scope is very limited (just for authentication) - clients should not have issue opening port to it
			-- Don't have/rely on security where adding a rate limit is part of security logic
			-- Security - chpt 26 of 2 scoops - go through it
			-- Don't have/rely on worflow 
			
			--|---- Basic thing for security:: see "Server Hardening", section 26.2 in pg.346 of 2 scoops:: Server hardening measures include but are not limited to things like setting up firewalls (help.ubuntu.com/community/UFW), changing your SSH port, and disabling/removing unnecessary services.
			--|---- **VERY VERY VERY IMPORTANT**: Good practice -- You should disable the HTML field autocomplete browser feature on fields that are gateways to payment. This includes credit card numbers, CVVs, PINs, credit card dates, etc. The reason is that a lot of people use public computers or their personal computers in public venues. In fact, consider changing the form field itself to PasswordInput. Also on that line - never store credit card data.
			--|---- Section 26.13 on Pg.357 about file handling. 
			--|----|---- Use CDN for file (I think this is more of a performance thing and to prevent getting your server drowned in false request).
			--|----|---- Since user uploaded file can have xss content, so don't have it open in browser by only allowing it to be downloaded - done by setting "content-disposition:attachment"
			--|----|---- When storing user-uploaded file, make sure they never get the "Executable" permission, even by self. Be very very careful if your workflow requires you to process files. Have it done somewhere else. Maybe also put upload size restriction; rate-limit based on ip / user-role-type; do virus scan. Wonder if a good thing would be to always encrypt file on disk - both for data security, plus the file gets jumbled so cannot execute. When storing user-file, never make the file-extension visible, instead have it only be in DB. Maybe also randomly chunk the file so it never makes sense on own (do so after encrypting entire file) -- do note that if you're accepting chunked files, then do a virus scan on combined files before continuing
			--|----|---- Use "magic" to inspect file-headers. For example: python has https://github.com/ahupp/python-magic
			--|---- **VERY VERY VERY IMPORTANT**: If working with custom user data which can be xml.. then realize that there can be a ddos attack done. See https://en.wikipedia.org/wiki/Billion_laughs_attack -- the article mentions that this attack can be done on any language with references (like, on YAML)
			--|---- **VERY VERY VERY IMPORTANT**: Displaying sequential primary keys is to be avoided because:
			--|----|---- They inform potential rivals or hackers of your volume
			--|----|---- By displaying these values we make it trivial to exploit InsecureDirectObjectReferences <-- important
			--|----|---- We also provide targets for XSS attacks
			--|---- **VERY VERY VERY IMPORTANT**: Links
			--|----|---- https://owasp.org/www-project-top-ten/
			--|----|---- https://wiki.mozilla.org/WebAppSec/Secure_Coding_Guidelines
			
			-- Don't take any html content from user. If you want to accept rick content, preferable look at markdown.. or open html in an iframe so it does not affect actual page.
			
			-- sql injection
			
			-- Handle xss and csrf issue
			--|---- Avoid inernal XSS. This comment is written earlier also::  If your DTO fields, path param, etc. has constrainst, like can be a-z,A-Z,0-9,_, then also realize these constraints in the field definition. DOn't just take any arbitrary string. Take only the ones expected. This can be useful in preventing internal xss, i.e., say as part of your logic, you are logging these field values, etc. - and then are running some daily background task, possibly to collect some metric. Since these are external user data - you shouldn't be trusting it.. and some user may try to pass in a script that inadvertantly gets run. This is internal XSS. The point is - don't trust any data comingf rom out till you've verified it. Similar also holds for fields containing html text from user.. Best would be to just get markup (and define an enum type column which highlights that the entry being read is of markup type, not html).. but if you do need to store html, then escape it as you form the request body itself. Also note that this promotes reusability, setting standards in project.
			--|---- In making a html safe.. define an "accept list" of tags that won't be escaped. DOn't instead define "reject list", i.e. list of tags that won't be accepted.. because you never know what might get missed. Having an accept list is better.
			--|---- use content security policy
			
			-- IDOR (insecure direct object reference) -- example can be checking parent child relation in REST call -- This is also a reason to use slug, and then define them in a random manner and not by joining fields because doing latter would make it guessable!
			-- SSRF (Server side request forgery) -- force a server to make request to unintended destination. This is bad because it breaks the trust in server. If there is any place in request-body or request-param where it is taking a url, that's bad. Can try putting in code in docx, pdf if you know it's going to get parsed/transformed and that can cause a hit on attacker webpage

			-- CSRF:
			--|---- double submit cookie -- send cookie in header and request body
			also ..but this can be bypassed by XSS
			--|---- Use Samesite cookie flag
			-- Don't rely just on referer or origin.. specially since referer can be changed
			
			-- RBAC -- if using, then make sure you always change access by role.. not anything else!!
			
			-- file vulnerabilities: If you take a file-path param from user. Appication goes there and picks up file
			--|---- abuse#1: use file-system pathway, like starting with ~, or ../../ - to go to other directories
			--|---- abuse#2: use filename like http://server.com/some-path - force application to do http call and download some bug, etc.
			--|---- In any cases, if downloading file, first do virus scan
			
			--|---- Is CSRF protection needed with GET call? -- Generally NO. 
			--|---- In most case, CSRF is restricted to attack where an attacking website tricks user to make a request that causes a mutation (in favor of attacker). However, when server returns a response --in these cases-- it will not have CORS' allow all origin header. So, even though a mutation is made, the response of it never goes back to attacker.
			--|---- HOWEVER consider the case where the server is of a business that is an utility-provider, say, like USPTO. Now, server can and should expect that different domains will be calling it - and so, it now needs to set CORS header to allow all origin. But this opens up a different problem in that even for non-mutating calls (GET call), the attacker can now see sensitive user data - and the webpage won't restrict it because CORS is effectively disabled. In such cases, it is necessary to have CSRF protection even on GET calls. 
			--|---- **VERY VERY IMPORTANT** The advantage of JWT comes here in that JWT created by one can be used by other.. token can be passed on front-end.. back-end.. etc. As long as token passes, the authentication continues and there's no need to worry of CSRF token. 
			--|----|---- RELATED: https://engineering.mixmax.com/blog/modern-csrf/
			--|----|---- RELATED: IF HOWEVER you're using cookie.. use "SAMESITE" setting on cookie
			--|---- Login CSRF
			--|---- If you also give an UI.. To prevent phishing, allow users to set an initial profile token that is showed when they log in. And allow this only on request coming from your UI domain (also adding for UI)
			--|---- Don't rely on cookie only for security. Use CSRFToken, or JWT. Using both is overkill. JWT does make you cloud ready
			
			-- Always virus check a downloaded file. If possible, also do a magic check
			
			-- Set RBAC access
			--|---- It might be good to group roles using prefix to apply it to certain subdomain (by defining permissions accordingly)
			--|---- Within one domain, aim to give only one role to a user. 
			--|---- Corresponding to the role, define a user profile such that it can be foreign key'd to by models exposed via views permitted to the role. If you have some common details to know from all user (like a base-profile), then just add those in your definition of "user" table. Have the profile based data be very specific to role. BUT.. when a "Group" is defined, it only defines Role (of RBAC) and not the profile. So, need a table to define "RoledProfile" for a user based on user-group (aka, user-role). This creates few more complications.. what happens if user changes roles. There may be model entries elsewhere in DB that refer to user profile.. but the old profile no longer remains. In such cases, it is better to instead never delete a profile and just soft-delete it. This prevent model relations from getting broken, keeps audit trail. This also means that when user changes role, they must also create a new profile and permissions must now be defined such that it looks both into role and whether the new profile is present and is active. A third question can be what happens if user changes profile back and forth -- and this relates to defining the unique constraint in the roledProfile table.
			--|----|---- One good thing about thinking in terms of "Profile" and RBAC rather than "user" is now you can think of how TAKE/SNITIZE request on all other DB tables look like for each "profile". Also, (profile + rbac) define permissions
			--|---- Even if you have hierarchy in role, its best to treat each role independent and define permission such that the role behaves hierarchical. So, in English, "the role is not hierarchical, but the permission associated to the role is hierarchical"
			--|---- Let's say we want to allow a "sub-optimal" authentication such that when confirmed by this route, then the user should only be allowed access to a small section of api(s) and not all of them. 
			--|----|---- FIRST.. Do check if this requirement can be removed, like, why the need to have such requirement??! An authenticated user is an authenticated user, so why the need for defining a "partial auhenticated user"! That being said, a genuine application can be for "user-as" kind of function - where you're logged in as a high role, but want to see how the api would look for some other role. 
			--|----|---- In such cases, define a temporary user oject, not in DB. Disable its save method, don't give it an id so that it cannot be saved. In filling the object fields from original one, only relevant fields are populated, and then it is assigned to lower group. Make a profile for it, and again don't give any id to it.. nor allow saving it. This ensures that the user object can be persisted. Also, it doesn't match any entries in DB and is totally ephemeral.. Still, be careful in related code. Now, to this user object - assign the new role, if its current role allow it to do so. Note that this would need a new utility method that should define role hierarchy, even though - as we defined above, it is a a flat list. Same utility can likely also be used by permission classes to identify the hierarchically up role that should also be granted access. If the role being requested canot be given, then retur 403
			--|----|---- Addition to above, in DB, separate out the table, so that the sub-portion that can be accessed by wider role user is a single table and the other restricted role access table is separate, joined to the former via a oneToOne relation.. and defining method preventing object access by other roles. How to define such a method is framework/language specific
			
			--cookies
			
			-- Ask sensitive data only at most once
			-- Store data as encrypted
			-- Use JWT or bearer token -- allows to go cloud ready
			-- PII Take and Sanitize (if you have separate data domain, this is easier)
			-- be careful not to leak status by error: Say you got "GET /root/a/{aId}/b" for some other criteria given in header or query param, and there's no match. A good answer could be to return [] with 200 status, saying - we don't want to tell you whether you're breaking any relation or not.. but hey, ythere's nothing matching. Similarly, "GET /root/a/{aId}/b/{bId} now returns 404". What if you do "POST /root/a/{aId}/b" and some header value or query param, but those don't hold reference relation with {aId}.. do you give 404? - can't because if you turn POST to GET, then resource exists and is [].. we just did above. Maybe you do a 400 saying that some value is not available - and rule become that anything 404 type in POST request gets returned with 400 instead. What if both reference relation don't hold and you don't have permission because you belong to {a2Id} not {aId}.. if the service instead returns just verifies your role but not your relation with {aId} as missing and you give some different error, then you leak information of relation. The thing to realize: First authenticate and authorize user, then check validity of url (or raise 404), then check relation between all other user/header data and url and raise 404 again (not 401 or 403) because other data provide additional context around user and so must be verified to form full "definition" of "user". Finally now, go to other status like 400, etc.
			--|---- Expanding further on above:consider url like: /root/a/{aId}
			--|----|---- realize that in sense of usage, the user would have done the following sequence of operations: /root, /root/a, /root/a/{aId}, /root/a/{aId}/b, /root/a/{aId}/b/{bId}. 
			--|----|---- /root wouldbe open to all users since it gives general info. 
			--|----|---- /root/a?{queryParams} - can also be opened to all users because there is no user and url link (..because there is no path param yet). It should return 200 with empty list if there's no data, or no data after filter, or user is not permitted any data.
			--|----|---- /root/a/{aId} (..and always without queryParams) - if it returns 404 for missing data, and 403 if user does not have relation with data, then it leaks info about existence of {aId} to the user. So, best is to return 403 or 401 on initial user validation, and then always return 404 either if {aId} does not exist, or {aId} exists but user does not have relation to it - to prevent leakig the existence of data. THIS IS TEH REASON why above it is suggested to just raise 401/403 for the first time, and then never raise it again, and just raise 404
			--|----|---- What about slugs received in process-serializer as part of request body? With same logic, that should always raise 400 error saying that the corresponding slug is not found. Do't say anything about not having permissions, etc.
			--|---- Consider the case of user adding a phone number. The user is verified (i.e. has email and logged via password), but the phone number is not verified. When user gives a phone number, either it is available (i.e. noone else has mentioned it) and user verifies it via 2fa code. BUT.. consider if there's an overlap where someone else has also mentioned same phoen number. Also, 2 more cases - other person has verified it, or other person has not verified it. when the user gives phone number - should they be told that there's a conflict? Correct answer is "No" - here, the user hasn't verified new number as belonging to them, so ideally it should never get any sort of communication back from the service. In essence, we are still expanding the verification domain.. and phone number is still not verified -- so it should not receive any commuication back! Should other people have given same phone number, should we tell them there's a conflict - Yes! - but only if the previous user has a verified phone number. If the previous user also has an unverified phone number, then don't send any message of "conflict". This gives an example of validating in an "exploding data domain" manner to ensure consistent response/sttus is sent so that any information is not leaked by failure.
			-- difference between authentication and authorization. Don't mix the two. Don't have "presence of an authentication method imply an authorization"
			
			-- be careful defining authorization for contract based relation: 
			--|---- Say you have a client and worker. When the two are in contract, then the client can see worker relation. But, if the contract is terminated for any reason, then the client shouldn't be able to do so. This means: (1) have a separate endpoint for clinet where worker's data is available. DOn't bundle it with some other endpoint allowing worker access to the data even after contract is over; (2) have a different authorization to acess the data via endpoint - don't just look for client-role on user making the request, but also if the client is allowed data acess for worker slug in path param. ALSO - best do this check as a permission and not within view because (a) this definition, as a permission is more apt, and, (b) view methods must be declarative in nature
			--|---- ALSO, a second design consideration for "time-bound-contract" type behavior is -- never give slug of one party out to another. Think of it like this.. if you enter into a contract with a 3rd party app to get your name, age from Facebook in exchange for taking some quiz.. but instead Facebook also gives it your "id/slug" and now this site becomes the go-to place where other 3rd party give a Facebook-id and get historical data. Then did Facebook do anything wrong? Yes -- **it shouldn't have given out your unique identifier that remains for perpetuity out to a service with which you have a temporary contract**. The better model design is to make a contract table that has foreign key from 3rd party app and from your profile. Now, what goes to 3rd party app is not you id/slug but the slug of contract. If they want your details.. they are given other fields like name, etc.. which can be general and not constrained to be unique. Here, you can control the degree of personalization on data outflow - maybe just give out first name not full name, maybe just the zip code and not exact coordinates. This way, the 3rd party cannot make something tailored to the user of given slug in their code -- which means, things can still get personalized but not individualized (separate topic: unless facebook offers that feature.. but then, its under their control which is still better than letting it out in wild). With the new model, you become "user associated with a contract", rather than "user with that slug.." which gives your application better control rather than have the providers take control.
			
			
			Monitoring:
			-- LIGHTSTEP - DISTRIBUTED TRACING:  **DOn't give actual implementation since you didn't write it
-- OpenTracing API
-- Defines "Span" : smallest unit of distributed trace
-- OpenTracing defines how to collect Span
-- LightStep is a provider that allows for collecting Spans
-- OpenCensus made by Google, now joined with OpenTracing to form OpenTelemetry (datadog style metrics along with openTracing)
-- can also run locally in developer mode and see the queries being made, and time taken
-- tracing does not depend on language being used.. as long as everyone uses openTracing, then everyone gets the span trace, even for 3rd party library

			--Sentry
			--|---- IMPORTANT: Make sure that in your Sentry and logs, you don't capture PII. You can probably capture id(s), roles, etc. - system info - but don't capture PII. This is a special gotcha regarding Sentry, because Sentry will capture full request body. Also, don't capture user tokens, cookies. If you see this as a limitation, then improve your logging and error handling. ALTERNATELY.. have a procedure in place to immediately identify source of error and then hide the PII in Sentry log.
			
			
			WRITING COMMENTS:
-- When in doubt search for google style guide on comment for that language.
-- Remember "comment maintenance" is also a burden that cannot be done by compiler - and takes team-time. So try to balance it
-- Maybe start by writing comment at top of file about what it does.
-- When writing comments, always start at top-most level "code-item", like Component (in JS) and Classes (in Python). This should be most general, and being highest in level, it should be more "declarative" in nature than imperative, i.e. highlight more on what/why rather than how. 
-- Then, go to specific functions "..which are exported" and write comments about their behavior. Include more description - but stick to what/why rather than how
-- Only inside code, if you are adding comment, then it's likely to be to explain something. BUT.. before doing so, consider the following:
--|----- Maybe change magic number by aptly named constant. Thus won't need to add comment
--|----- A good place for comment in if you feel that the function has stages/chunks but you don't want to make the code unnecessarily small by pre-emptively breaking it
			-- Related: Give proper variable name, don't skimp!!

			Retry logic:
			-- Making a unified client; Ambassador pattern
			
			
			Discuss code arrangement based on above
			-- differentiate between view utilities vs just utilities; any method requiring handle to request should be related to view and nothing else. Even though serializer deals with request body, the view should create a context for it and pass related arguments. Don't have the serializer directly call request or read from it
			-- differentiate between global utils vs package level utils
			-- positioning of constants
			--|---- One thing that can be placed in global scale constants file (and not constants defined in each sub module) is if the project is importing a 3rd party library, and the utilities in it use some string formats, etc. Since the import applies globally, so it comes to be that any constants being defined for use therein must also be global. Best is to start by defining things locally, and then move them to global if it gets reused at different places - to have a DAG structure. This does require extra work in being agile to refactor.. but is good practice as it keeps context in understanding what a constant does and in what scope is it used.
			-- Inheritance vs composition
			-- abstraction vs encapsulation: DIfferent for python, JS vs for java
			-- long file vs short file
			
			
			
			Testing -- do feature test; make mock for background task; make mock for 3rd party api. Do multi-feature test along lines of business workflow 
			
			Cover these 3 points, and link to prod-prod
			Rule of 3 to identify refactor
			-- Foolish consistency is hogoblin
			-- if needed, go for speed, and cover tech debt later. Business first
			
			
			DB:
			-- one jira ticket should result in 1 migration. on the migration file, add ticket name and briefly what it does - for easy tracking/identifying. No need to go full detail as git blame can be used to do so
			-- say that DB design should not be the first thing to start with.. start with service and then come to database design!!
			-- explain why normalize data. When denormalize data and that it can be done even in SQL. However, if doing so, then check consistency of referential keys
			-- as best practice add both table and column level comments
			
			-- You can change character-set in MySQL columns.. but best to have charset as utf8mb4  **IMPORTANT**: Not utf8.. in MySQL, utf8 is utf8mb3 (3 bytes of data). Even worse, it can drop string data  MySQL historically set around 767 bytes for indexing. Since most string columns are 255 string size, so MySQL decided that theyll assign 3 bytes per string. So, if there is a larger data sent.. it gets dropped
			
			--  Alter table: 2 things to note. Both relate to fact that Alter table, when executed, makes copy of original, then changes references to refer to the new table. So:
1) If trying to do multiple ALTER TABLE commands, best to collect all of them in a single statement t prevent performance degradation by multiple operations
2) Historically, Alter table would lock the table while alteration is being done. Recent versions have improved it by trying to queue the changes and in the mean time allowing external reads to happen.. but this is still slow!
			
			-- LIMIT command has another form that takes 2 values, 1 is offset and other is the number of entries returned - but under covers this is not performance.. as it first selects everything and then throws offset data out. Better way to paginate is to first filter using WHERE clause, and then use limit.. and then change the input to WHERE for next iteration accordingly.
			
			-- Non updatable columns
			-- Always keep separate numeric id. Don't go for string or natural key since logic may change in future.
			-- SELF-JOIN: table joining on itself. Good for join to get a hierarchy or time-window relation. In this case, for disambiguation, use AS clause
			-- See StackOverflow https://stackoverflow.com/questions/3856164/sql-joins-vs-sql-subqueries-performance  in that joins can be faster than sub select
			-- Single service should be owner of data and this should give rise to data domain design. Once again, follow DAG -> link to "maintain DAG" in Prod Prod.. which itself is to ensure a good communication on "happens before" relation. Data domain driven design. Data ownership driven design.
			-- Single application as being the owner of db and it accesses it
			-- Maintain read only replica for speed
			-- When doing updates, ensure the existing table structure works. Also check that adding data from new works
			-- Using Enum rather than standard table creates problems in that old migration files would need changing
			
			1) If you have, say, "N entries" out of which you want only 1 to be active at a time.. then one way is to have say an "active" field that can be true for just one entry. For all others, it should be null. The "null" isn't used in unique and so it behaves as if there's just one active at a time.
			--|---- HOWEVER, it feels like cheating and is not a portable behavior (https://stackoverflow.com/questions/20154033/allow-null-in-unique-column). A downside is that if within the group, you want to set an active value to null and some other null value to active, then you'll have to defer consistency till after transaction - which MySQL doesn't allow. So, some restrictions may come - but still, there's workaround.. use 2 transactions - if doing so, set the intermediate value to false, and change it back to null. From application perspective it should still work. Also, for the set of unique values that can be put, try to keep it limited, best if this is done on a boolean field so it can take on true/false value. 
			--|---- Another approach can be to enforce the check using optimistic or pessimistic locking. If using optimistic lock, it may help to have a top level record that defines the version.
			
			-- DB unique constraint in design :: Start by adding as strong of a constraint as possible.. relax later if need comes. Don't start with a weak unique constraint (i.e. one based on multiple fields rather than just a single field)
			--|---- Design it with view to enforce uniqueness of created data, hence it solves problem of race condition in creating new data - where DB raises error. 
			--|---- Does this (https://dba.stackexchange.com/questions/210949/why-does-this-query-result-in-deadlock) mean that in absence of unique constraint adding / updating 2 entries can cause deadlock!! - another reason to always keep separate numeric id
			
			-- Standard data table (vs Enum)
			
			-- Discuss "select for update" vs "optimistic locking"
			--|--- As part of better understanding: understand isolation levels: https://stackoverflow.com/questions/4034976/difference-between-read-commited-and-repeatable-read#:~:text=Repeatable%20read%20is%20a%20higher,unchanged%2C%20and%20available%20to%20read.
			--|---- Above link : https://dba.stackexchange.com/questions/210949/why-does-this-query-result-in-deadlock - Particularly, interesting is observation that even if one is doing create/update in single transaction - that can cause deadlock! Sure, it should have used better uniqueness constraints.. but over here, "select-for-update" works but the optimistic locking won't help!!
			--|---- Update calls can be done pessimistically or optimistically: See https://medium.com/@hakibenita/how-to-manage-concurrency-in-django-models-b240fed4ee2  REALIZE that: 
			--|----|---- (i) pessimistic calls using "select_for_update", even if it works - does not guarantee that data won't be overwritten. For example: User-1 reads data-version-0, and updates to new-value-v11. User-2 concurrently reads data-version-0, but updates to value-v12. If not done properly, select_for_update will just guarantee that the 2 updates happen one after another, which means it can go in any manner. One can set "nowait=True" so an exception is returned.. still, it does not guarantee the order of updates -- meaning, maybe User-2 updated to value-v12 just because he saw v0 to begin with. If he saw v11 (from user-1), maybe user-2 would have put something else. 
			--|----|---- (ii) For above reason, and because it is easier.. using Optimistic locking is better. However, realize that doing so means you'll have to do 2 reads. The "medium.com" example above shows as if its doing just 1 call.. but in order to pass "self.version" in get() call, it'll first have to query db to get recent version. Another option -- get "version" to use as a hidden param (in form), or as a header, or as query param. BECAUSE -- thinking again of UX.. when user makes an update.. they are doing so based on some data - so it must be communicated back to server about what data they were using. [[Instead of version, one may also seek to use timestamp]]
			--|---- ABOUT "SELECT FOR UPDATE":
			--|----|---- As mentioned above, this still cannot solve unintended overwrites!! - For that, need optimistic locking / versioning
			--|----|---- Django doc: https://docs.djangoproject.com/en/dev/ref/models/querysets/#select-for-update
			--|----|----|----- **VERY VERY IMPORTANT**: NOTE: it says that if queryset has select_related(), then those rows are also locked! This in itself could be a good feature for certain use cases.. See https://stackoverflow.com/questions/10935850/when-to-use-select-for-update
			--|----|---- See https://www.brightbox.com/blog/2013/10/31/on-mysql-locks/ for different locks that can happen. THUS, when using "select_for_update", best restrict yourself ONLY to lookup by "id" only - so that you just do row level locks
			--|---- A UX problem with optimistic locking is that if it fails, end user will get a message to fill form again that can be frustrating. Sure, UI can be improved.. but better still, wraps optimistic failures in retry logic. Also, related question - just to prevent user annoyance, do you want them to mistakenly overwrite someone else's data. Plus, UI frameworks can save form-data if the call is unsuccessful.. you can return HTTP 409. so why not?!
			--|----  If using optimistic lock, and operation can touch multiple rows in some sub-table, then it may help to have a top level record that defines the version.
			--|---- To mention/clarify again: select_for_update is best done when you want to lock a table during update. For example, say you only want certain column to have sequential numbers. The way to getting it would be to get the current table size and use it to get next number. Thus, synchronization is inherently needed and using optimistic locking only may not be helpful - because in creating new row, it'll have a version of 0. And you're not modifying old entry's version.. so optimistic lock will end up creating overwritten entries.. but not so if done using select_for_update
			--|----|---- ALSO.. If your object has a lot of concurrent updates you are probably better off with the pessimistic approach. If you have updates happening outside the ORM (for example, directly in the database) the pessimistic approach is safer. If your method has side effects such as remote API calls or OS calls make sure they are safe (Taken from https://hakibenita.com/how-to-manage-concurrency-in-django-models)
			--|---- **IMPORTANT**: Optimistic Locking is particularly good/useful for cases where partial update of row cannot be done. Thus the row completely updates in each operation. In such cases, row-versioning in optimistic locks prevent update on an old version of data. Note that optimistic locks are good to use when there is a low write/concurrent-update rate. Also, having this paradigm requires both an understanding from client and server side. Client must understand that they're trying to update a "version" data and that the operation can fail in which case they'll need to retry. However, depending on use-case server side may add some retry logic. On the other hand, select for update pauses the execution rather than causing failures like optimistic locks - so it is good for high write rate. Implementing optimistic locking is not available out of box so it needs extra coding work. RELATED: (i) Realize that model.save() can have additional referential checks before data is saved. Those checks get bypassed if using queryset.update() used for optimistic locking; (ii) since queryset.update() is getting invoked, then model's save() signal will not get invoked; (iii) save() method has an argument "update_fields" that can additionally be used for performance gains and to enable concurrent update of different portions of model. This feature can be used instead of update() for optimistic locking. **ALSO** don't worry about optimistic/pessimistic lock unless you're starting to see db contention.

			
			-- Be careful defining cascade behavior for deletion. safest is always to disable cascade - so that if such a behavior is needed, then application specifically does so
			-- Use of null valued columns for group uniqueness. BUT, they will cause error if you want to change a different value in group to be true within single transaction.
			-- When having db model where lower table has columns that override value of column in a higher table -- then: (1) use column-name in lower table as `{high-column-name}_override` add @property named `{high-column-name}` in lower table that defines the value as using the overriden value if available else using the higher table value.
			
			-- before finalizing the table design, do a data domain analysis (give link to matin fowler's page). Idea is to (1) ensure that you have identified and properly separated all tables into individual data domains. The data domains should be less than equal to count of individual profiles/roles. (2) used the domain to identify the ownership of each table under TAKE/SANITIZE by each profile. (3) The "id/slug" within tables for a data domain that don't overlap with another domain should not form part of tables in overlap region (example.. worker having contract - don't leak worker slug/id)
			
			-- UTF-8 based security issue + using utfmb4 type in MySQL:
* https://mathiasbynens.be/notes/mysql-utf8mb4
* https://speakerdeck.com/mathiasbynens/hacking-with-unicode
			
			
			-->
		</nav>
		<main tabindex="0"></main>
		<footer></footer>
		<script src="https://code.jquery.com/jquery-3.5.1.min.js" integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin="anonymous"></script>
		<script type="module">
			import { onUploadNoteFileChange, onDownloadNoteFile, onDocumentLoad } from './scripts.js';
			$(document ).ready(onDocumentLoad);
			$("main").on("change", "#note-file-upload-input", (event) => onUploadNoteFileChange(event.target.files[0]));
			$("main").on("click", "#note-file-download-button", onDownloadNoteFile);
		</script>
	</body>
<html>