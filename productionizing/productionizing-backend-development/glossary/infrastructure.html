<!--
    HTML book section: Infrastructure related term glossary to Productionizing Backend Development, by Kunj Prasad on Github.
    Copyright (C) 2020  Kunj Prasad

    This program is free software: you can redistribute it and/or modify
    it under the terms of the GNU Affero General Public License as published
    by the Free Software Foundation, either version 3 of the License, or
    (at your option) any later version.

    This program is distributed in the hope that it will be useful,
    but WITHOUT ANY WARRANTY; without even the implied warranty of
    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
    GNU Affero General Public License for more details.

    You should have received a copy of the GNU Affero General Public License
    along with this program.  If not, see <https://www.gnu.org/licenses/>.
-->
<article>
	<h2>Infrastructure related terms</h2>
	<p>This page lists the various terms that relate to infrastructure and other applications needed to support the deployment and hosting of a web application. Since this book focuses on backend development, it may not use all of the terms. Nonetheless, it is important for a backend software developer to be aware of these terms and what they mean.</p>
	
	<h3>Server</h3>
	<p id="1608393835">See article about server on <a href="https://en.wikipedia.org/wiki/Server_(computing)" target="_blank">Wikipedia</a>. It is a combination of software and hardware that reads an incoming request and returns a response. The entity making the request is called a "client". In backend development, developers leverage an existing software framework that handles common task of reading a generic incoming request and sending a generic response, and leaves gaps for pluggable custom code to provide specific business logic. Also note, a server should be able to handle multiple simultaneous request from each client in an independent manner, i.e., without letting any request know about others. So, how does a server know about the client to which it should send a response to? In short, it uses client address and port to define a connection over which the response is sent (see <a href="https://unix.stackexchange.com/questions/75011/how-does-the-server-find-out-what-client-port-to-send-to" target="_blank">here</a>).</p>
	
	<h3>Cloud</h3>
	<p id="1608394258">See article about cloud computing on <a href="https://en.wikipedia.org/wiki/Cloud_computing" target="_blank">Wikipedia</a> and on <a href="https://azure.microsoft.com/en-us/overview/what-is-cloud-computing/#cloud-deployment-types" target="_blank">Microsoft Azure</a>. Consider youself as opening an e-business and also having a corresponding software code to achieve the goal. However, for the business to begin, the software code needs to deployed to servers. And then comes a whole lot of hardware and software maintenance, upgrades, security patches. If your business is a hit, you'd want more servers and quickly. Maybe a lot more users show up at certain time of day or year, then you'd like to have more servers for just that small time. What if your business does not work out - now you want to sell the servers for cost recovery! Needing a server is like wanting to go between 2 places in a city. Owning a server is like buying a car. It is good for certain use cases and it comes with its constraints, like, you need to know how to drive a car. Cloud service is like a cab service or ride share. You don't bother with anything about driving or car but you're paying a small fraction towards someone else's profit out of your own. It is good during initial stages and can be revisited later.</p>
	
	<h3>Load balancer</h3>
	<p id="1608395384">See article about load balancer on <a href="https://en.wikipedia.org/wiki/Load_balancing_(computing)" target="_blank">Wikipedia</a>, <a href="https://www.citrix.com/glossary/load-balancing.html" target="_blank">Citrix</a> and <a href="https://www.nginx.com/resources/glossary/load-balancing/" target="_blank">Nginx</a>. A load balancer, simply put, balances load of incoming request to different backend servers. The main goal is to have each server get an uniform workload for request processing. This ensures optimal user experience by not having them wait simply becaue the request got routed to a busy server while another server is free. Different routing strategies can be configured to distribute load and to enable various response caching strategies suitable to business needs. Additionally, HTTPS termination can also be done at this layer before sending the request to server. It can also send periodic health checks to backing servers and remove routing traffic to inactive servers.</p>
	
	<h3>Database</h3>
	<p id="1608527531">See article about databases on <a href="https://en.wikipedia.org/wiki/Database" target="_blank">Wikipedia</a> and on <a href="https://www.oracle.com/database/what-is-database/" target="_blank">Oracle</a>. The primary reason to use a database can be tracked down to HTTP requests being inherently stateless. Thus, a temporary data store is needed to save user data that can be retrieved and modified at a later time. To enable storing large amounts of data while keeping the read and write operations effiient, a database table can be "sharded" or "partitioned" into smaller units that deal with the table data in a smaller range. Additionally, "replica" tables can be made to provide fail-safe if the primary owner of a shard data does down.</p>
	
	<h4>Relational database, or RDBMS</h4>
	<p id="1608528587">See article about relational database on <a href="https://en.wikipedia.org/wiki/Relational_database" target="_blank">Wikipedia</a>. The "relational" prefix is used because the data captured-from or presented-to user is stored within the database in a logically connected manner, among various database tables, as identified on the basis of interaction among these tables. Many relational database systems use SQL (Structured Query Language) for querying and maintaining the database.</p>
	
	<h4>Non relational database, or NqSQL database</h4>
	<p id="1608529017">See article about non relational database on <a href="https://en.wikipedia.org/wiki/NoSQL" target="_blank">Wikipedia</a>, <a href="https://www.mongodb.com/nosql-explained" target="_blank">MongoDB</a> and <a href="https://aws.amazon.com/nosql/" target="_blank">AWS</a>. Unlike the relational databases that require the data to be in a well-formed structure that matches with the definition of tables, these provide ability to store semi-structured documents with flexibility to add columns within those documents. In breaking off from the requirement posed by SQL of having a properly defined table definition to be present and that all entries in a table comply with that structure, these databases label themselves as No-SQL databases. The use case for NoSQL database generally overlaps with need for extremely quick processing , which then requires removing costly join operations between database tables. Hence, use of NoSQL database overlaps with use of de-normalized table design (discussed under architecture glossary). However, it should be noted that the two are separate, and there's nothing stopping from using denormalized tables in relational database.</p>
	
	<h4>Cache</h4>
	<p id="1608530099">See article about cahces on <a href="https://en.wikipedia.org/wiki/Cache_(computing)" target="_blank">Wikipedia</a>, <a href="https://aws.amazon.com/caching/" target="_blank">AWS</a>. The simplest way to think about a cache is to consider it as an in-memory Map object, i.e. for a given key, it returns a value. Cached data is kept in memory so that it can be used rather than making a database or network call. However, since memory hardware is much more expensive that a disk storage, so cached data should be such that it is small in size and still provide performance gains by getting reused multiple times. Identifying what data should be cached depends on the business use case. keep small. The process of removing data from cache is called "evication". This can be done by removing the "least recently used" data, or the "least frequently used" data. Reuse of cache data is called its "hit rate". Optimizing a cache involves analyzing the way cahce is used to ensure a high hit rate (to get same data being queried multiple times and so it is returned from cache rather than from database) with a low eviction rate (to get the benefit of cache use rather than terms getting constantly evicted and then re-loaded). A cache can also be distributed wherein one machine hosting the cache returns data corresponding to a particular "shard" or "partition" of entire dataset, and/or can host a "replica" to shard data owned by a different machine. Even though "Cache" is mentioned under the database section in here, note that a cache is a general term. A cache can be added at various stages, like, to store data pulled from database, or to store user profile for corresponding cookie, or even <a href="https://developer.mozilla.org/en-US/docs/Web/HTTP/Caching" target="_blank">at front end to reuse previous responses</a>. However, when adding cache, ensure that stale data is not being sent to user in a manner unexpected by user.</p>
	
	<h3>Code repository</h3>
	<p id="1608564873">See article about code repository on <a href="https://en.wikipedia.org/wiki/Repository_(version_control)" target="_blank">Wikipedia</a>, <a href="https://bitbucket.org/product/code-repository" target="_blank">BitBucket</a> and in a <a href="https://huspi.com/blog-open/software-code-repositories" target="_blank">blog at HUSPI</a>. The simplest way is to think of it as a database for enterprise codebase. It should allow different team mebers to collaborate on the project and access different versions fo code as incremental changes are committed by team members. Particularly, note that the version control system can either be distributed (like, Git or Mercurial) or centralized (like, Subversion or CVS). Having worked on both, I personally prefer the distributed system.</p>
	
	<h3>Continuous Delivery (or, CD), and Continuous Integration (or, CI)</h3>
	<p id="1608565983">See article about continuous develivery on <a href="https://continuousdelivery.com/" target="_blank">ContinuousDelivery</a>, <a href="https://en.wikipedia.org/wiki/Continuous_delivery">Wikipedia</a> and <a href="https://aws.amazon.com/devops/continuous-delivery/" target="_blank">AWS</a>. For a team to keep up with continuous delivery, every member ensures that the code available in repository is always free of faults and production ready. One important component to achieve this is to have continuous integration where developers merge there code in same repository and which is free of conflicts. See article about continuous integration on <a href="https://en.wikipedia.org/wiki/Continuous_integration" target="_blank">Wikipedia</a>, <a href="https://www.atlassian.com/continuous-delivery/continuous-integration" target="_blank">Atlassian</a> and <a href="https://aws.amazon.com/devops/continuous-integration/" target="_blank">AWS</a>. Within the repository itself, sub-folders could be made to handle parallel development, merging features and identifying code for release. For example, using <a href="https://stackoverflow.com/questions/698313/what-is-trunk-branch-and-tag-in-subversion" target="_blank">trunk, branch and tag folders in subversion</a>, or using <a href="https://www.atlassian.com/git/tutorials/comparing-workflows/gitflow-workflow" target="_blank">Git flow</a>, etc. Additionally, one must use some application that verifies on each commit that the newly added code does not break existing features and previous build. This is where <a href="https://www.jenkins.io/" target="_blank">Jenkins</a> may help.</p>
	
	<h3>Virtualization versus Containerization</h3>
	<p id="1608570000">For articles on virtualization, see <a href="https://en.wikipedia.org/wiki/Virtualization" target="_blank">Wikipedia</a>, <a href="https://www.vmware.com/solutions/virtualization.html" target="_blank">VMWare</a> and <a href="https://opensource.com/resources/virtualization" target="_blank">OpenSource</a>. To enable, full resource utilization, virtualization creates multiple copies of same/different OS on same hardware. As a side benefit, the system software and all installations can now be copied and easily moved to new hardware. Contrast this to containerization where different applications are deployed on a hardware having same OS, but in separate, independent "containers" that are isolated from each other. For articles on containeriation, see <a href="https://en.wikipedia.org/wiki/Containerization" target="_blank">Wikipedia</a>, <a href="https://www.docker.com/resources/what-container" target="_blank">Docker</a> and <a href="https://www.citrix.com/glossary/what-is-containerization.html" target="_blank">Citrix</a>. <a href="https://docs.docker.com/get-started/overview/" target="_blank">Docker</a> and <a href="https://kubernetes.io/docs/concepts/overview/what-is-kubernetes/" target="_blank">Kubernetes (which is a container manager)</a> are two common applications used in containerization. With the development in containerization, it has clawed off some of the functionalities that were previously done via virtualization. However, they both can be used in tandem. See <a href="https://www.burwood.com/blog-archive/containerization-vs-virtualization" target="_blank">one of the blogs</a> discussing the similarities an differences between the two.</p>
	
	<h3>System Monitoring</h3>
	<p id="1608572592">This can include a vast array of utilities to monitor system performance to identify if the servers are performing in an expected manner, i.e. expected levels of CPU usage, network usage, no memory leaks, etc. It can also include sending periodic healthcheck requests to different servers to identify if they are up. Common tools are Unix statsd, Graphite, Nagios, Datadog, etc.</p>
	
	<h4>SLO, SLI and Error budget</h4>
	<p id="1608575849">See articles on Atlassian about <a href="https://www.atlassian.com/incident-management/kpis/sla-vs-slo-vs-sli" target="_blank">SLO, SLI</a> and <a href="https://www.atlassian.com/incident-management/kpis/error-budget" target="_blank">Error budget</a>. These are performance indicators to identify if the business application (i.e. the service) is behaving in a manner expected for the business. Once the application makes it to production environment and is available to users, the service level objectives (SLOs) must be identified such that it mirrors business objectives. System monitoring tools can be used to obtain service level indicators (SLIs), i.e. the acual performance level for a service. The difference between SLI and SLO can be used to define error budget, i.e. how much error a service can take before it fails to meet the SLO. Depending on business requirement, there can be multiple SLOs. </p>
</article>
