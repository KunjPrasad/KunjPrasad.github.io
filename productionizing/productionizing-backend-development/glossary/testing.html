<!--
    HTML book section: Testing related terms glossary to Productionizing Backend Development, by Kunj Prasad on Github.
    Copyright (C) 2020  Kunj Prasad

    This program is free software: you can redistribute it and/or modify
    it under the terms of the GNU Affero General Public License as published
    by the Free Software Foundation, either version 3 of the License, or
    (at your option) any later version.

    This program is distributed in the hope that it will be useful,
    but WITHOUT ANY WARRANTY; without even the implied warranty of
    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
    GNU Affero General Public License for more details.

    You should have received a copy of the GNU Affero General Public License
    along with this program.  If not, see <https://www.gnu.org/licenses/>.
-->
<article>
	<h2>Testing related terms</h2>
	<p>This page lists the various terms related to testing of a software application. This isn't an exhaustive list but only contains terms that I feel are of most importance. For a more detailed lists of different tests, see this article at <a href="https://www.atlassian.com/continuous-delivery/software-testing/types-of-software-testing" target="_blank">Atlassian</a>. Tests are an integral part of software development. Thankfully, there isn't much that can go wrong in writing tests as long as some of the basics of testing are covered. This book does not cover all of the terms because related details can easily be obtained from basic search. Nonetheless, it is important to be aware of these terms and what they mean. Heads up, beyond unit and functional test, there may not be whole lot of consensus on definition of other tests. My suggestion would be to focus on what a test does and why it is important, rather than on what it is named, and then go with the name used by the team.</p>
	
	<h3>Unit test</h3>
	<p id="1608654300">See article about unit testing on <a href="https://en.wikipedia.org/wiki/Unit_testing" target="_blank">Wikipedia</a>. In writing a code, many public methods get defined. To put simply, unit tests are to test that all the public methods in the code are behaving as expected. These tests "mock" out any dependency public methods used (..within the code being tested) to focus the test only on the specific code being tested. It may also involve advanced mocking to mock system methods (like, getting system date) because even these are external dependencies to the code being tested.</p>
	
	<h4>Code coverage</h4>
	<p id="1608654390">A metric closely asociated with unit test is "code coverage". See article about code coverage on <a href="https://en.wikipedia.org/wiki/Code_coverage" target="_blank">Wikipedia</a>. It is the percentage of code that has been tested by unit tests. While a high code coverage does not necessarily means that the application is bug-free (..and that's why other tests also exist, as mentioned below), a low coverage does erode confidence in ability of the code to be bug-free. From experience, I notice that aspring for a 85% or better of code coverage is a good starting point. Code coverage further breaks into 2 parts: line coverage and branch coverage. Branch coverage includes tests covering the ranched code execution, like, loops, if-else, switch-case, exception, etc.</p>
	
	<h3>Functional test</h3>
	<p id="1608654990">See article about functional testing on <a href="https://en.wikipedia.org/wiki/Functional_testing" target="_blank">Wikipedia</a>. Functional testing is used to verify that a user interacting with the web application sees an expected response. So, this testing occurs on the request/response level. Almost all web application development frameworks also provide testing utilities to set up a mock server to which requests can be made as if done by a real user. The mock server processes this request and returns a response. The test compares the returned response to verify if it is matches the expectation. To facilitate this testing, the utilities also provide or allow adding mock databases.</p>
	
	<h4>Test data provider</h4>
	<p id="1608655325">Since functional tests aim to mimick a user interaction as closely possible, it is necessary that the test data added in database also mirrors exactly to what it might be in a real use case. Hence, it is a good idea to use a centralized data provider that prepares consistent and correct data for each functional test. However, depending on the overall business process, there may be one or more of such providers.</p>
	
	<h4>Background job test</h4>
	<p id="1608655586">The complete web application may involve background processing in addition to handling web requests from users. If these tasks modify database entries made in some previous request, or prepare data without which future requests may fail, then they have an indirect interaction with the user and so, they must be tested. Testing of these tasks may require making custom test utilities that simulate as if the background task was run, or actually run them on the mock test server. Test assertions must be made on application state (i.e., most likely, database entries) both before and after the task was run.</p>
	
	<h4>Third party api test</h4>
	<p id="1608656300">If the web application makes a third party api call, either as part of a request processing, or from a background task, then such behaviors of the web application must also be tested. Testing third party integration most likely will require making custom class to mock the actual api being used. Testing may involve either verifying that a call was made to these APIs. If making a call to these APIs can also cause a change in application state (i.e., most likely, database entries), then test assertions must be made to verify the behavior. It may also be the case that these third party api calls are being done within the background tasks. In this case, the test must invoke the background task and verify the effects of third party api call.</p>
	
	<h3>Integration test</h3>
	<p id="1608656723">In real life, the interaction of a user with a web-application will never be restricted to just a single request. The data exchanged between user and web application in some request will also get directly or indirectly exchanged in some different call. Or, maybe, it opens or closes future options. The simplest example is that a user may be not be allowed to access web application without having a profile. After a profile is made, they may be allowed access to only some requests. When their authorization status is changed, they can then access more requests. And if their profile is suddenly deleted, they will again be unable to access the application. A integration test can be seen as a multi-functional test intends to perform a sequence of calls to cover a business processing chain and verify that each step works as intended and with no unexpected side-effects.</p>
	
	<h3>Test-driven development, or TDD</h3>
	<p id="1610066426"><a href="https://en.wikipedia.org/wiki/Test-driven_development" target="_blank">Test-driven development (TDD)</a> is a software development process relying on software requirements being converted to test cases before software is fully developed, and tracking all software development by repeatedly testing the software against all test cases. This is opposed to software being developed first and test cases created later. The motivation for doing so is that unlike the actual code, it is much easier to write functional and integration level tests based on the requirements of the task since the tests are written against user request and expected response. Also, it becomes much easier to identify how much of the overall development effort is pending or if there's a particular thorny issue in the eature being developed that is slowing down the development. This can be done by simply looking at failing test cases. Since the first steps in feature development involves adding test cases, it keeps out personal bias and need to push out a feature that may prevent a developer from thoroughly testing the feature against all edge cases.</p>
	<p id="1610066882">Despite its advantages, there are also cons of adopting TDD. The primary con is that if same developer is writing the code and also the test cases, then there will always be a bias, regardless of what happens first. Hence, best use of TDD comes if two developers can simulatenously profress on a feature: one of them writing the code and other writing the tests. This ensures that at a later time when the code is run on test, then the developer can get an unbiased feedback by looking at failing tests if certain functions have not been added. For this to happen, the coding requirements must have been solidified before any development begins. However, in real world, this is not always assured. As development proceeds, new issues might be found, or new interactions between different business services may be identified that need handling. Maybe the structure of request and response body isn't defined when the coding started, or, maybe the development is moved to code branch for a separate ticket. It may also be the case the feature being developed is experimental in nature and likely to change in future. In this case, TDD doubly hurts because it takes developer time away from feature development and later on, when the behavior is changed, then the old tests need to deleted and rewritten. TDD, by its nature, can test for positive cases and not test strongly for negative cases because feature requirements generally cover when new behavior is needed and not what's not needed. Without having any perspective of the underlying code, it is impossible to test all set of negative test cases that can arise. Also, if goal of entire exercise is to verify that code has been written properly, and meets the technical levels expected by team, and sufficient positive and negative tests have been made, then a robust code review practice is also necessary and TDD by itself isn't sufficent. So, while TDD may be a good practice, it shouldn't be used as a silver bullet.</p>
	
	<h3>End-to-end test</h3>
	<p id="1608657637">The simplest way to think of an end-to-end testing is as an integration test, but without any mocks. These tests are run on actual deployed software application, actual third party api and background task. Additional effort is needed to ensure that database has entries which help in doing the end-to-end testing but which do become accessible to the user. These are usually done in a quality check environment to ensure that the actual web application, without any mocked tests and components, works as intended. When a product is getting deployed for the first time, it may be useful to do this test once in production environment before opening the product to users.</p>
	
	<h3>Load test</h3>
	<p id="1608658136">See this article about load testing on <a href="https://en.wikipedia.org/wiki/Load_testing" target="_blank">Wikipedia</a>. Load test involves simulating an actual request load from expected count of different users to the system, and to then verify if the system behaves as expected. The metric of most importance from a business viewpoint is whether the latency in serving the response is below a maximum target, and if the environment can handle the requests in expected manner. Preparation for load test involves creating a testing environment with similar count and specification of server as would run in production environment. The group of servers that emulate users should be such that they are able to simulate multiple users making simultaneous request to test server without themselves getting limited by context switching. Also, the network latency between these user-emulating servers and the one getting load tested should be considered when analyzing the results. As part of test result analysis, it should be verified the servers getting load tested have low latency in responding back and also have optimal cpu and memory usage, while returning expected successful response. Of particular importance is the fact that this the only class of test that can identify concurrency related bugs (like, race conditions in code, or third party api, or any other connection causing unexpected behavior) which would otherwise not show up in any other tests. While load tests can be skipped by new business because they will likely not see a huge workload for some time, it is an absolute must for important web applications relied on by many.</p>
	
	<h3>Code review</h3>
	<p id="1608659343">One of the earliest, most available and yet the hardest test to pass before a code gets included within the business application is peer code review. A team that does not provide venues for proper and continous code review to its developers almost instantly looses a lot of credibility in creating a proper application code. In garnering multiple questions and opinions about the code, the code review process ensures that standards set by team are met. It also provides a quick venue for other developers to keep pace with changes occurring at a difference place in code.</p>
	
	<h4>Code linter</h4>
	<p id="1608660132">See article about linter on <a href="https://en.wikipedia.org/wiki/Lint_(software)" target="_blank">Wikipedia</a>. Linters are useful to automaticaly ensure that team follows a defined coding style and are probably the first of code-reviewes. More importantly though, they automatically identify many of the <a href="https://en.wikipedia.org/wiki/Code_smell" target="_blank">code smells</a> and some possible bugs. So, having a linter automatically trigger is very useful to identify and catch bugs in the early stages of code development. Some code repositories management system allow configuring hooks so that linters are run before a code is committed (example: see <a href="https://levelup.gitconnected.com/how-to-run-eslint-using-pre-commit-hook-25984fbce17e" target="_blank">here for ESLint run over Javascript code during commit in Git</a>). It is highly suggested to use the linters as much possible.</p>
</article>
